{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Motivation\n",
    "Nowadays social media are filled with figurative and creative language, including irony. The understanding irony in a speech is required for human reasoning and communication, As a result, studies on understanding sarcasm and irony have been started in machine learning researches. Especially, irony has important implications for natural language processing (NLP) tasks, which aim to understand and produce human language. Some potential applications of automatic irony detection are Tasks requiring semantic analysis, author profiling, online harassment detection, and sentiment analysis. Also, understanding irony helps us to have a better analysis of social media, and the opinion of people about many things. For this project, we chose this task because it was an interesting, useful, well-defined task. And also, we wanted to check our knowledge from the course in this task. Alongside our knowledge from the course, we aimed to learn a new kind of sequence analysis method, named Temporal Convolutional Network (TCN). And we also wanted to test this method too and compare it with other models we had learned in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
=======
    "## Approach\n",
>>>>>>> 005224f38f63ea13ba7130deecb48d8f0e9df051
    "### Embedding\n",
    " The size of our training data is rather small (3834 sentences), so we used pre-trained embeddings as the input of our model. We used two different pre-trained model and compared their result with each other. First, we used the last four hidden layers of BERT pre=trained model as the embedding for the words of the sentences. we used BERT tokenizer to split tokens in the sentences and then, extract the embeddings for them. For example, if we have the sentence, \"Here is the sentence I want embeddings for.\", the tokens will be 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.'. Each layer of BERT has 726 dimensions, so when we concatenate last four hidden layers the dimension of our embeddings become 3072. \n",
    " Second, we used the pre-trained Glove model which was trained on 2B tweets. It has 27B tokens and 1.2M uncased vocab. There are four version for this embeddings based on their dimension. we used 100 dimensional version of it in our experiments. Before extracting word embeddings, we used Glove pre-processing script for twitter data, and then create space between word and punctuations. We tokenized the sentences by spliting by space delimiter. \n",
    " \n",
    "### Model\n",
    "We tried two different types of models to classify the tweets. First, we used a TCN based mode. We tried different numbers of convolutional layers, kernel sizes, strides, and hidden sizes. The reported results which had the best results have 4 layers of convolutional layers, 3 kernel size, 1 stride and 600  and 100 hidden size for BERT and Glove cases, respectively. Also, Convolutions in layer i had 2^i dilation. After the convolutional layers, our model select the channels of last word of the last hidden layer. Then we applied a linear layer with the activation of Sigmoid to get the probability of being irony for the input tweet.\n",
    "![tcn](figures/tcnmodel.png \"TCN based model\")\n",
    "Second, we used different types of RNN models to extract a hidden representation for the sentence and similar to the previous case, we applied a linear layer and Sigmoid to extract the probability of being irony. We tried LSTM, Bidirectional LSTM, GRU, and Bidirectional GRU as the RNN model. the reported results have 1 RNN layer because increasing the number of layers did not get better results. It also has 600 and 100 hidden size for the BERT and Glove embedding cases, respectively.\n",
<<<<<<< HEAD
    "![rnn](figures/rnn.png \"RNN based model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The task of this project is coming from SemEval-2018 Task3. In this task given a dataset of tweets, the task is to train a model to detect irony tweets from non-irony tweets. So this task is a classification task with just two classes, Irony and Non-Irony. Dataset of this project is the official dataset of this task.\n",
    "\n",
    "Primary dataset for this task was provided from crawling twitter searching for hashtags e.g. #sarcasm, #irony, and #not. 3000 tweets provided by this search. Human experts have checked the tweets and 2,396 of these tweets were really irony, and the rest were not. For contest they have removed irony related hashtags from tweets. They also added 1792 other non-irony tweets to make irony-nonirony balance. So dataset includes 2,396 irony and 2,396 non-irony tweets. Then they devided data to 80% of training samples (3,833 samples), and 20% of test samples (958 test samples). In additional cleaning step they have removed some ambiguous samples from test data, so finally test file includes 784 samples. They were two subtasks, detecing irony, and detecting the type of the irony. We just used the irony detection task (Subtask A) for this project.\n",
    "\n",
    "some samples from gathered tweets (Irony related hashtags are removed in final dataset):\n",
    "\n",
    "    1. Go ahead drop me hate, I'm looking forward to it. (Irony)\n",
    "    2. I love waking up with migraines #not (Irony)\n",
    "    3. Is Obamacare Slowing Health Care Spending? #NOT (Non-Irony)\n",
    "    4. And I then my sister should be home from college by time I get home from babysitting. And it's payday. THIS IS A GOOD FRIDAY (Non-Irony)\n",
    "\n",
    "Train file is:\n",
    "data/datasets/train/SemEval2018-T3-train-taskA.txt\n",
    "\n",
    "Test file is:\n",
    "ref/SemEval2018-T3_gold_test_taskA_emoji.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
=======
    "![rnn](figures/rnn.png \"RNN based model\")\n",
    "\n",
>>>>>>> 005224f38f63ea13ba7130deecb48d8f0e9df051
    "## Code\n",
    "For the TCN model, we used some parts of codes of the paper \"An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling\" by Shaojie Bai, J. Zico Kolter and Vladlen Koltun. We used the part of code which was for Word-level Language Modeling task which can be find [here](https://github.com/locuslab/TCN/tree/master/TCN/word_cnn). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Experimental Setup\n",
    "\n",
    "We used officially introduced metrics in the contest. Accuracy and F1 score are the main metrics of comparing models. The code evaluate.py calculates all these parameters.\n",
    "\n",
    "$$\\operatorname{accuracy}=\\frac{\\text {true positives }+\\text {true negatives}}{\\text {total number of instances}}$$\n",
    "$$\\text { precision }=\\frac{\\text { true positives }}{\\text { true positives }+\\text { false positives }}$$\n",
    "\n",
    "$$\\text {recall}=\\frac{\\text {true positives}}{\\text {true positives }+\\text { false negatives }}$$\n",
    "\n",
    "$$F_{1}=2 \\cdot \\frac{\\text { precision } \\cdot \\text {recall}}{\\text { precision }+\\text {recall}}$$\n",
    "\n",
    "We have implemented and tested some models:\n",
    "\n",
    "<ol>\n",
    "  <li>TCN</li>\n",
    "  <li>LSTM/Bidirectional LSTM</li>\n",
    "  <li>GRU/Bidrectional GRU</li>\n",
    "</ol>\n",
    "\n",
    "For each of these models we have used two types of pretrained embeddings:\n",
    "\n",
    "<ol>\n",
    "  <li>BERT</li>\n",
    "  <li>Glove</li>\n",
    "</ol>\n",
    "\n",
    "Making BERT embeddings takes long time for whole dataset, as the embeddings are context based and we can not use ready vectors for each word. So, model saves embeddings after the first time model finding embeddings. Each time for training new model it just uses the saved model.\n",
    "\n",
    "For comparision we compared our results with two baselines of the contest and top 5 competitors models. Considering F1 score, GRU using BERT pretrained embeddings could achieve second place in this contest!\n",
    "\n",
    "Two baselines of the contest are random binary model, which just randomly specifies the classes, and SVM model using tf-idf vectors. Top 5 competitors used some models and several techniques to improve their results. A short explanation of their works to improving the reults has came in \"SemEval-2018 Task 3: Irony Detection in English Tweets\" report.\n",
    "\n",
    "We used a GTX 1080 Ti for training our models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
=======
>>>>>>> 005224f38f63ea13ba7130deecb48d8f0e9df051
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran our experiment 5 times for each method and reported the best one.\n",
    "Here our methods are TCN, GRU, GRU-bidirection, LSTM and LSTM-bidirection. Our methods are reported on both BERT and Glove language models. In addition, for the comparison, we have included the top 5 models and baseline methods from the competition as well.\n",
    "The methods were sorted based on F1 score in the competition. We also followed this convention for our methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border = 1> <tr><td>method</td><td>acc</td><td>precision</td><td>recall</td><td>F1</td></tr><tr><td>THU_NGN</td><td>0.735</td><td>0.630</td><td>0.801</td><td>0.705</td></tr><tr><td>NTUA-SLP</td><td>0.732</td><td>0.654</td><td>0.691</td><td>0.672</td></tr><tr><td>WLV</td><td>0.643</td><td>0.532</td><td>0.836</td><td>0.650</td></tr><tr><td>NLRPL-IITBHU</td><td>0.661</td><td>0.551</td><td>0.788</td><td>0.648</td></tr><tr><td>NIHRIO</td><td>0.702</td><td>0.609</td><td>0.691</td><td>0.648</td></tr><tr><td>GRU_BERT</td><td>0.737</td><td>0.653</td><td>0.702</td><td>0.685</td></tr><tr><td>Bi_GRU_BERT</td><td>0.731</td><td>0.663</td><td>0.563</td><td>0.658</td></tr><tr><td>LSTM_BERT</td><td>0.739</td><td>0.702</td><td>0.591</td><td>0.642</td></tr><tr><td>TCN_BERT</td><td>0.673</td><td>0.571</td><td>0.714</td><td>0.634</td></tr><tr><td>Bi_LSTM_BERT</td><td>0.726</td><td>0.692</td><td>0.556</td><td>0.617</td></tr><tr><td>GRU_Glove</td><td>0.666</td><td>0.562</td><td>0.711</td><td>0.628</td></tr><tr><td>LSTM_Glove</td><td>0.682</td><td>0.591</td><td>0.650</td><td>0.619</td></tr><tr><td>Bi_GRU_Glove</td><td>0.681</td><td>0.588</td><td>0.653</td><td>0.619</td></tr><tr><td>Bi_LSTM_Glove</td><td>0.684</td><td>0.603</td><td>0.592</td><td>0.597</td></tr><tr><td>TCN_Glove</td><td>0.634</td><td>0.531</td><td>0.662</td><td>0.589</td></tr><tr><td>Unigram SVM</td><td>0.635</td><td>0.532</td><td>0.659</td><td>0.589</td></tr><tr><td>Random</td><td>0.503</td><td>0.373</td><td>0.373</td><td>0.373</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [['method', 'acc', 'precision', 'recall', 'F1'],\n",
    "        ['THU_NGN', '0.735', '0.630', '0.801', '0.705'],\n",
    "        ['NTUA-SLP', '0.732' , '0.654', '0.691', '0.672'],\n",
    "        \n",
    "        ['WLV' ,'0.643', '0.532', '0.836', '0.650'],\n",
    "        \n",
    "        ['NLRPL-IITBHU', '0.661', '0.551', '0.788', '0.648'],\n",
    "        \n",
    "        ['NIHRIO', '0.702', '0.609', '0.691', '0.648'],\n",
    "        \n",
    "        ['GRU_BERT', '0.737', '0.653', '0.702' ,'0.685'],\n",
    "        ['Bi_GRU_BERT', '0.731' ,'0.663', '0.563', '0.658'],\n",
    "        ['LSTM_BERT' ,'0.739' , '0.702', '0.591' , '0.642'],\n",
    "        ['TCN_BERT' ,'0.673' , '0.571' , '0.714' , '0.634'],\n",
    "        ['Bi_LSTM_BERT', '0.726', '0.692', '0.556', '0.617'],\n",
    "        \n",
    "        ['GRU_Glove', '0.666', '0.562', '0.711', '0.628'],\n",
    "        ['LSTM_Glove', '0.682', '0.591', '0.650', '0.619'],\n",
    "        ['Bi_GRU_Glove', '0.681', '0.588' ,'0.653', '0.619'],\n",
    "        ['Bi_LSTM_Glove', '0.684', '0.603', '0.592' ,'0.597'],\n",
    "        ['TCN_Glove', '0.634', '0.531', '0.662', '0.589'],\n",
    "        ['Unigram SVM', '0.635', '0.532', '0.659', '0.589'],\n",
    "        ['Random', '0.503', '0.373', '0.373', '0.373'],\n",
    "       ]\n",
    "display(HTML(\n",
    "    #'<html><head><style>table, th, td {border: 1px solid black;}</style></head><body>'\n",
    "   \"<table border = 1> <tr>{}</tr></table>\".format(\n",
    "       '</tr><tr>'.join(\n",
    "           '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data)\n",
    "       )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is shown in the table above, our GRU and LSTM models based on BERT language model have achieved the best accuracy among the competition models. Moreover, we have the best precision with LSTM on BERT language model. For the recall and F1 score we didn't beat the best model, but we have achieved the F1 score to get the second place in the competition.\n",
    "The reason for the lower F1 score could be having low recall score. It seems that achieving better recall values would be feasible with some parameter tuning, which we did not have so much time to do it perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we used pre-trained embeddings and compared TCN and some variations of RNN models for detecting irony tweets. For the future we have some ideas to augment our data by adding or removing some unimportant words. We can also use attention to specify the importance of hidden units we use to generate the output. We could also concatenate embeddings from different langauge models to capture differnt embeddings in various contexts and types of sentences, but it needs a good tokenizer that can work for different language models, which seems hard and time consuming, and we didn't have the oppurtunity to check this direction.At last, as mentioned, parameter fine-tuning is also a good method to improve the performance, but again we did not have so much time to investigate it completely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
<<<<<<< HEAD
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
=======
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
>>>>>>> 005224f38f63ea13ba7130deecb48d8f0e9df051
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
