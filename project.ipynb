{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "Nowadays social media are filled with figurative and creative language, including irony. The understanding irony in a speech is required for human reasoning and communication, As a result, studies on understanding sarcasm and irony have been started in machine learning researches. Especially, irony has important implications for natural language processing (NLP) tasks, which aim to understand and produce human language. Some potential applications of automatic irony detection are Tasks requiring semantic analysis, author profiling, online harassment detection, and sentiment analysis. Also, understanding irony helps us to have a better analysis of social media, and the opinion of people about many things. For this project, we chose this task because it was an interesting, useful, well-defined task. And also, we wanted to check our knowledge from the course in this task. Alongside our knowledge from the course, we aimed to learn a new kind of sequence analysis method, named Temporal Convolutional Network (TCN). And we also wanted to test this method too and compare it with other models we had learned in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "### Embedding\n",
    " The size of our training data is rather small (3834 sentences), so we used pre-trained embeddings as the input of our model. We used two different pre-trained model and compared their result with each other. First, we used the last four hidden layers of BERT pre=trained model as the embedding for the words of the sentences. we used BERT tokenizer to split tokens in the sentences and then, extract the embeddings for them. For example, if we have the sentence, \"Here is the sentence I want embeddings for.\", the tokens will be 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.'. Each layer of BERT has 726 dimensions, so when we concatenate last four hidden layers the dimension of our embeddings become 3072. \n",
    " Second, we used the pre-trained Glove model which was trained on 2B tweets. It has 27B tokens and 1.2M uncased vocab. There are four version for this embeddings based on their dimension. we used 100 dimensional version of it in our experiments. Before extracting word embeddings, we used Glove pre-processing script for twitter data, and then create space between word and punctuations. We tokenized the sentences by spliting by space delimiter. \n",
    " \n",
    "### Model\n",
    "We tried two different types of models to classify the tweets. First, we used a TCN based mode. We tried different numbers of convolutional layers, kernel sizes, strides, and hidden sizes. The reported results which had the best results have 4 layers of convolutional layers, 3 kernel size, 1 stride and 600  and 100 hidden size for BERT and Glove cases, respectively. Also, Convolutions in layer i had 2^i dilation. After the convolutional layers, our model select the channels of last word of the last hidden layer. Then we applied a linear layer with the activation of Sigmoid to get the probability of being irony for the input tweet.\n",
    "![tcn](figures/tcnmodel.png \"TCN based model\")\n",
    "Second, we used different types of RNN models to extract a hidden representation for the sentence and similar to the previous case, we applied a linear layer and Sigmoid to extract the probability of being irony. We tried LSTM, Bidirectional LSTM, GRU, and Bidirectional GRU as the RNN model. the reported results have 1 RNN layer because increasing the number of layers did not get better results. It also has 600 and 100 hidden size for the BERT and Glove embedding cases, respectively.\n",
    "![rnn](figures/rnn.png \"RNN based model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The task of this project is coming from SemEval-2018 Task3. In this task given a dataset of tweets, the task is to train a model to detect irony tweets from non-irony tweets. So this task is a classification task with just two classes, Irony and Non-Irony. Dataset of this project is the official dataset of this task.\n",
    "\n",
    "Primary dataset for this task was provided from crawling twitter searching for hashtags e.g. #sarcasm, #irony, and #not. 3000 tweets provided by this search. Human experts have checked the tweets and 2,396 of these tweets were really irony, and the rest were not. For contest they have removed irony related hashtags from tweets. They also added 1792 other non-irony tweets to make irony-nonirony balance. So dataset includes 2,396 irony and 2,396 non-irony tweets. Then they devided data to 80% of training samples (3,833 samples), and 20% of test samples (958 test samples). In additional cleaning step they have removed some ambiguous samples from test data, so finally test file includes 784 samples. They were two subtasks, detecing irony, and detecting the type of the irony. We just used the irony detection task (Subtask A) for this project.\n",
    "\n",
    "some samples from gathered tweets (Irony related hashtags are removed in final dataset):\n",
    "\n",
    "    1. Go ahead drop me hate, I'm looking forward to it. (Irony)\n",
    "    2. I love waking up with migraines #not (Irony)\n",
    "    3. Is Obamacare Slowing Health Care Spending? #NOT (Non-Irony)\n",
    "    4. And I then my sister should be home from college by time I get home from babysitting. And it's payday. THIS IS A GOOD FRIDAY (Non-Irony)\n",
    "\n",
    "Train file is:\n",
    "data/datasets/train/SemEval2018-T3-train-taskA.txt\n",
    "\n",
    "Test file is:\n",
    "ref/SemEval2018-T3_gold_test_taskA_emoji.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "For the TCN model, we used some parts of codes of the paper \"An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling\" by Shaojie Bai, J. Zico Kolter and Vladlen Koltun. We used the part of code which was for Word-level Language Modeling task which can be find [here](https://github.com/locuslab/TCN/tree/master/TCN/word_cnn). We will explain some important parts of code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can specify which type of embedding we use (BERT or Glove) by lmodel args option. For the BERT embedding we have data_generator which extract all BERT embeddins for train, validation, and test data. At data_generator function we will check if we have saved the embedding in Corpus object. If we did not, we will create them by running create_embeddings function for train and test files. We had original train and test files and we had the other ones that converted emojies to their name. We used the first one to extract labels and the second one to extract the words of sentences. For each sentence, we did the preprocessing first and then get BERT embedding by get_bert_embedding function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(args):\n",
    "    if os.path.exists(args.data + \"/corpus\") and not args.corpus:\n",
    "        corpus = pickle.load(open(args.data + '/corpus', 'rb'))\n",
    "    else:\n",
    "        corpus = Corpus(args.data,\n",
    "                        '/train/SemEval2018-T3-train-taskA.txt',\n",
    "                        '/goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt',\n",
    "                        '/test_TaskA/SemEval2018-T3_input_test_taskA.txt')\n",
    "        pickle.dump(corpus, open(args.data + '/corpus', 'wb'))\n",
    "    return corpus\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    def __init__(self, path, trainfile, testfile_emoji, testfile_without_emoji):\n",
    "        self.train_embeddings, self.train_labels = self.create_embeddings(path, trainfile, trainfile)\n",
    "        self.test_embeddings, self.test_labels = self.create_embeddings(path, testfile_emoji, testfile_without_emoji, is_test=True)\n",
    "        self.valid_embeddings = self.test_embeddings[:int(len(self.test_embeddings)*0.1)]\n",
    "        self.valid_labels = self.test_labels[:int(len(self.test_labels)*0.1)]\n",
    "\n",
    "    def create_embeddings(self, path, file_emoji, file_witout_emoji, is_test=False):\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        break_point = 10\n",
    "        with open(path + file_witout_emoji, encoding=\"utf8\") as fp:\n",
    "            lines = fp.readlines()\n",
    "            lines = lines[1:len(lines)]\n",
    "            for i, l in enumerate(lines):\n",
    "                if i > break_point:\n",
    "                    break\n",
    "                line = l.split(\"\\t\")\n",
    "                sent_index = 1 if is_test else 2\n",
    "                sentence = line[sent_index]\n",
    "                sentence = pre_process(sentence)\n",
    "                sent_embed = get_bert_embedding(sentence)\n",
    "                sent_embed = torch.stack(sent_embed)\n",
    "                embeddings.append(sent_embed)\n",
    "\n",
    "        with open(path + file_emoji, encoding=\"utf8\") as fp:\n",
    "            lines = fp.readlines()\n",
    "            lines = lines[1:len(lines)]\n",
    "            for i, l in enumerate(lines):\n",
    "                if i > break_point:\n",
    "                    break\n",
    "                line = l.split(\"\\t\")\n",
    "                labels.append(torch.FloatTensor([float(line[1])]))\n",
    "\n",
    "        return embeddings, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing phase, we changed links to '< quote >' and mentions to '@somebody'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_links(text_input):\n",
    "    words = text_input.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if 'http' not in word:\n",
    "            new_words.append(word)\n",
    "        else:\n",
    "            new_words.append('<quote>')\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "def change_mentions(text_input):\n",
    "    words = text_input.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if '@' != word[0]:\n",
    "            new_words.append(word)\n",
    "        else:\n",
    "            new_words.append('@somebody')\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "def pre_process(text_input):\n",
    "    text_input = text_input.lower()\n",
    "    text_input = change_links(text_input)\n",
    "    text_input = change_mentions(text_input)\n",
    "    return text_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in get_bert_embedding function, first we added [CLS] and [SEP] token to the beginning and the end of sentence. Then we extracted the tokens by tokenize funcction of BERT tokenizer. Then we loaded the pre-trained bert-base-uncased model and extracted the last 4 layers of it as the embeddings for all of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text_input):\n",
    "    marked_text = \"[CLS] \" + text_input + \" [SEP]\"\n",
    "    text_tokens = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(text_tokens)\n",
    "    segments_ids = [1] * len(text_tokens)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "    segments_tensors = torch.tensor([segments_ids]).to(device)\n",
    "\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    token_vecs_cat = []\n",
    "\n",
    "    for token in token_embeddings:\n",
    "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "        token_vecs_cat.append(cat_vec)\n",
    "\n",
    "    return token_vecs_cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for Glove embedding, first we load glove embedding file in load_glove_embedding. Then, in create_embeddings function we extracted tokens by tokenize function. In the tokenize function, we used the python version of Ruby pre-processing code for twitter data by Glove. We splited some punctuations from their neighbouring words and then we tokenize the sentences by the space delimiter. Finally, we extracted 100d Glove embeddings for each token in get_glove_embedding function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GloveCorpus:\n",
    "\n",
    "    def __init__(self, path, trainfile, testfile_emoji, testfile_without_emoji):\n",
    "        self.glove_embeddings = self.load_glove_embedding()\n",
    "        self.train_embeddings, self.train_labels = self.create_embeddings(path, trainfile, trainfile)\n",
    "        self.test_embeddings, self.test_labels = self.create_embeddings(path, testfile_emoji, testfile_without_emoji, is_test=True)\n",
    "        self.valid_embeddings = self.test_embeddings[:int(len(self.test_embeddings)*0.1)]\n",
    "        self.valid_labels = self.test_labels[:int(len(self.test_labels)*0.1)]\n",
    "\n",
    "    def create_embeddings(self, path, file_emoji, file_witout_emoji, is_test=False):\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        with open(path + file_witout_emoji, encoding=\"utf8\") as fp:\n",
    "            lines = fp.readlines()\n",
    "            lines = lines[1:len(lines)]\n",
    "            for i, l in enumerate(lines):\n",
    "                line = l.split(\"\\t\")\n",
    "                sent_index = 1 if is_test else 2\n",
    "                sentence = line[sent_index]\n",
    "                tokens = self.tokenize(sentence)\n",
    "                sent_embed = self.get_glove_embedding(tokens)\n",
    "                sent_embed = torch.stack(sent_embed)\n",
    "                embeddings.append(sent_embed)\n",
    "\n",
    "        with open(path + file_emoji, encoding=\"utf8\") as fp:\n",
    "            lines = fp.readlines()\n",
    "            lines = lines[1:len(lines)]\n",
    "            for i, l in enumerate(lines):\n",
    "                line = l.split(\"\\t\")\n",
    "                labels.append(torch.FloatTensor([float(line[1])]))\n",
    "\n",
    "        return embeddings, labels\n",
    "\n",
    "    def hashtag(self, text):\n",
    "        FLAGS = re.MULTILINE | re.DOTALL\n",
    "        text = text.group()\n",
    "        hashtag_body = text[1:]\n",
    "        if hashtag_body.isupper():\n",
    "            result = \" <hashtag> {} <allcaps> \".format(hashtag_body)\n",
    "        else:\n",
    "            result = \" \".join([\" <hashtag> \"] + re.split(r\"(?=[A-Z])\", hashtag_body, flags=FLAGS))\n",
    "        return result\n",
    "\n",
    "    def allcaps(self, text):\n",
    "        text = text.group()\n",
    "        return text.lower() + \" <allcaps> \"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        FLAGS = re.MULTILINE | re.DOTALL\n",
    "        # Different regex parts for smiley faces\n",
    "        eyes = r\"[8:=;]\"\n",
    "        nose = r\"['`\\-]?\"\n",
    "\n",
    "         # function so code less repetitive\n",
    "        def re_sub(pattern, repl):\n",
    "            return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "\n",
    "        text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" <url> \")\n",
    "        text = re_sub(r\"/\",\" / \")\n",
    "        text = re_sub(r\"@\\w+\", \" <user> \")\n",
    "        text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \" <smile> \")\n",
    "        text = re_sub(r\"{}{}p+\".format(eyes, nose), \" <lolface> \")\n",
    "        text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \" <sadface> \")\n",
    "        text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \" <neutralface> \")\n",
    "        text = re_sub(r\"<3\",\" <heart> \")\n",
    "        text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \" <number> \")\n",
    "        text = re_sub(r\"#\\S+\", self.hashtag)\n",
    "        text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat> \")\n",
    "        text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong> \")\n",
    "\n",
    "        ## -- I just don't understand why the Ruby script adds <allcaps> to everything so I limited the selection.\n",
    "        # text = re_sub(r\"([^a-z0-9()<>'`\\-]){2,}\", allcaps)\n",
    "        text = re_sub(r\"([A-Z]){2,}\", self.allcaps)\n",
    "\n",
    "        text = text.lower()\n",
    "        text = self.pre_process(text)\n",
    "        \n",
    "        return text.split()\n",
    "\n",
    "    def pre_process(self, text):\n",
    "        changed = True\n",
    "        before_signs = ['.', ',', '!', '?', ')', ':', '#', '\"', '*', '(', '|', '=',]\n",
    "        after_signs = ['(', '.', ':', '\"', '*', ')', '|', '=']\n",
    "        while(changed):\n",
    "            changed = False\n",
    "            tokens = text.split()\n",
    "            for i, token in enumerate(tokens):\n",
    "                if changed:\n",
    "                    break\n",
    "                for sign in before_signs:\n",
    "                    if token.find(sign, 1) > -1:\n",
    "                        ind = token.find(sign,1)\n",
    "                        tokens = tokens[:i] + [token[:ind], token[ind:]] + tokens[i+1:]\n",
    "                        text = ' '.join(tokens)\n",
    "                        changed = True\n",
    "                if changed:\n",
    "                    break\n",
    "                for sign in after_signs:\n",
    "                    if token.find(sign) > -1 and token.find(sign) < len(token)-1:\n",
    "                        ind = token.find(sign)\n",
    "                        tokens = tokens[:i] + [token[:ind+1], token[ind+1:]] + tokens[i+1:]\n",
    "                        text = ' '.join(tokens)\n",
    "                        changed = True\n",
    "                if changed:\n",
    "                    break\n",
    "                if token.find(\"n't\") > -1 and len(token) > 3:\n",
    "                    ind = token.find(\"n't\")\n",
    "                    tokens = tokens[:i] + [token[:ind], \" n't \", token[ind+3:]] + tokens[i+1:]\n",
    "                    text = ' '.join(tokens)\n",
    "                    changed = True\n",
    "                if changed:\n",
    "                    break\n",
    "                if token.find(\"'\", 1) > -1 and token.find(\"n't\") == -1:\n",
    "                    ind = token.find(\"'\", 1)\n",
    "                    tokens = tokens[:i] + [token[:ind], token[ind:]] + tokens[i+1:]\n",
    "                    text = ' '.join(tokens)\n",
    "                    changed = True\n",
    "                if token.find('_') > -1 and len(token) > 1:\n",
    "                    ind = token.find('_')\n",
    "                    tokens = tokens[:i] + [token[:ind], token[ind+1:]] + tokens[i+1:]\n",
    "                    text = ' '.join(tokens)\n",
    "                    changed = True\n",
    "        return text\n",
    "\n",
    "    def load_glove_embedding(self):\n",
    "        glove_embeddings = {}\n",
    "        with open('data/embeddings/glove.twitter.27B/glove.twitter.27B.100d.txt', encoding=\"utf8\") as f:\n",
    "            for i, l in enumerate(f):\n",
    "                if i % 100000 == 0:\n",
    "                    print('load glove line', i)\n",
    "                tokens = l.split()\n",
    "                glove_embeddings[tokens[0]] = torch.FloatTensor([float(x) for x in tokens[1:]])\n",
    "        return glove_embeddings\n",
    "\n",
    "    def get_glove_embedding(self, tokens):\n",
    "        embeddings = []\n",
    "        for token in tokens:\n",
    "            if token in self.glove_embeddings:\n",
    "                embeddings.append(self.glove_embeddings[token])\n",
    "            else:\n",
    "                print(token)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then by model args option we decide which neural network we should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.model == 0:\n",
    "    model = TCN(args.emsize, 1, num_chans, dropout=dropout, kernel_size=k_size)\n",
    "if args.model == 1:\n",
    "    model = LSTM_classifier(input_size = args.emsize, output_size = 1, hidden_size = args.nhid)\n",
    "if args.model == 2:\n",
    "    model = LSTM_classifier_bidirectional(input_size = args.emsize, output_size = 1, hidden_size = args.nhid)\n",
    "if args.model == 3:\n",
    "    model = GRU_classifier(input_size = args.emsize, output_size = 1, hidden_size = args.nhid)\n",
    "if args.model == 4:\n",
    "    model = GRU_classifier_bidirectional(input_size = args.emsize, output_size = 1, hidden_size = args.nhid)\n",
    "if args.model == 5:\n",
    "    model = RNN_classifier(args.emsize, 1, hidden_size=args.nhid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the definition of different models (TCN, LSTM, BiLSTM, GRU and BiGRU) we used in this project. TCN use TemporalConvNet block, a linear layer and sigmoid. The other ones use LSTM, Bidirectional LSTM, GRU and Bidirectional GRU plus a linear layer and sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size\n",
    "                 , output_size, num_channels,\n",
    "                 kernel_size=2, dropout=0.3):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "\n",
    "        self.decoder = nn.Linear(num_channels[-1], output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Input ought to have dimension (N, C_in, L_in), where L_in is the seq_len; here the input is (N, L, C)\"\"\"\n",
    "        y = self.tcn(input.transpose(1, 2)).transpose(1, 2)\n",
    "        y = self.decoder(y[:,-1,:])\n",
    "        y = self.sigmoid(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class LSTM_classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(LSTM_classifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(input.shape)\n",
    "        output,(_, _) = self.lstm(input)\n",
    "        y = self.decoder(output[:,-1])\n",
    "        y = self.sigmoid(y)\n",
    "        return y\n",
    "\n",
    "class LSTM_classifier_bidirectional(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(LSTM_classifier_bidirectional, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first = True, bidirectional = True)\n",
    "        self.decoder = nn.Linear(2  * hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(input.shape)\n",
    "        output,(_, _) = self.lstm(input)\n",
    "        y = self.decoder(output[:,-1])\n",
    "        y = self.sigmoid(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GRU_classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(GRU_classifier, self).__init__()\n",
    "        self.GRU = nn.GRU(input_size, hidden_size, batch_first = True)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(input.shape)\n",
    "        output,_ = self.GRU(input)\n",
    "        y = self.decoder(output[:,-1])\n",
    "        y = self.sigmoid(y)\n",
    "        return y\n",
    "\n",
    "class GRU_classifier_bidirectional(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(GRU_classifier_bidirectional, self).__init__()\n",
    "        self.GRU = nn.GRU(input_size, hidden_size, batch_first = True, bidirectional = True)\n",
    "        self.decoder = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(input.shape)\n",
    "        output,_ = self.GRU(input)\n",
    "        y = self.decoder(output[:,-1])\n",
    "        y = self.sigmoid(y)\n",
    "        return y\n",
    "\n",
    "class GRU_classifier_mlayers(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(GRU_classifier_mlayers, self).__init__()\n",
    "        self.GRU = nn.GRU(input_size, hidden_size, batch_first = True, num_layers = num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(input.shape)\n",
    "        output,_ = self.GRU(input)\n",
    "        y = self.decoder(output[:,-1])\n",
    "        y = self.sigmoid(y)\n",
    "        return y\n",
    "\n",
    "class RNN_classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(RNN_classifier, self).__init__()\n",
    "        self.RNN = nn.RNN(input_size, hidden_size, batch_first = True)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        #input = input.permute(1, 0, 2)\n",
    "        #print(input.shape)\n",
    "        output, _ = self.RNN(input)\n",
    "        y = self.decoder(output[:,-1])\n",
    "        y = self.sigmoid(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the definition of TemporalConvNet which we brought from codes of the paper we mentioned before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In train function for each sentence in train_data we extract the output of the model and then optimize the parameters of our model based on the Binary Cross Entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    global train_data\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    cor_num = 0\n",
    "    indices = torch.randperm(len(train_data))\n",
    "    for batch_idx, ind in enumerate(indices):\n",
    "        data, label = train_data[ind][0], train_data[ind][1]\n",
    "        data = data.view(1, data.size(0), data.size(1))\n",
    "\n",
    "        label = label.view(1, label.size(0))\n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        if torch.abs(output - label).item() < 0.5:\n",
    "            cor_num += 1\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        loss.backward()\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation and training data, we call evaluate funciton which extract the output of our model from those datasets and return the loss and accuracy of the results. We also save the outputs based on a threshold on the output probablity of being irony for each sentence in predictions-taskA.txt file if this function was caled with the save_output argument as True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source, save_output=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    processed_data_size = 0\n",
    "    cor_num = 0\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(data_source):\n",
    "            data = data.view(1, data.size(0), data.size(1))\n",
    "            label = label.view(1, label.size(0))\n",
    "            if args.cuda:\n",
    "                data = data.cuda()\n",
    "                label = label.cuda()\n",
    "            output = model(data)\n",
    "            outputs.append(output)\n",
    "\n",
    "            if torch.abs(output - label).item() < 0.5:\n",
    "                cor_num += 1\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            processed_data_size += data.size(1)\n",
    "\n",
    "        if save_output:\n",
    "            with open('res/predictions-taskA.txt', 'w+') as f:\n",
    "                for output in outputs:\n",
    "                    if output <= 0.5:\n",
    "                        f.write('0\\n')\n",
    "                    else:\n",
    "                        f.write('1\\n')\n",
    "\n",
    "        return total_loss / len(data_source), (float)(cor_num)/len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the model which have the best accuracy on the validation dataset and after all of epochs, we load the best saved model and run that on the test data and report the loss and accuracy on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if val_acc >= val_tacc:\n",
    "    with open(\"model.pt\", 'wb') as f:\n",
    "        print('Save model!\\n')\n",
    "        torch.save(model, f)\n",
    "    best_vacc = val_acc\n",
    "    \n",
    "# Load the best saved model.\n",
    "with open(\"model.pt\", 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "# Run on test data.\n",
    "test_loss, test_acc = evaluate(test_data, save_output=True)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test acc {:1.2f}'.format(\n",
    "    test_loss, test_acc))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Setup\n",
    "\n",
    "We used officially introduced metrics in the contest. Accuracy and F1 score are the main metrics of comparing models. The code evaluate.py calculates all these parameters.\n",
    "\n",
    "$$\\operatorname{accuracy}=\\frac{\\text {true positives }+\\text {true negatives}}{\\text {total number of instances}}$$\n",
    "$$\\text { precision }=\\frac{\\text { true positives }}{\\text { true positives }+\\text { false positives }}$$\n",
    "\n",
    "$$\\text {recall}=\\frac{\\text {true positives}}{\\text {true positives }+\\text { false negatives }}$$\n",
    "\n",
    "$$F_{1}=2 \\cdot \\frac{\\text { precision } \\cdot \\text {recall}}{\\text { precision }+\\text {recall}}$$\n",
    "\n",
    "We have implemented and tested some models:\n",
    "\n",
    "<ol>\n",
    "  <li>TCN</li>\n",
    "  <li>LSTM/Bidirectional LSTM</li>\n",
    "  <li>GRU/Bidrectional GRU</li>\n",
    "</ol>\n",
    "\n",
    "For each of these models we have used two types of pretrained embeddings:\n",
    "\n",
    "<ol>\n",
    "  <li>BERT</li>\n",
    "  <li>Glove</li>\n",
    "</ol>\n",
    "\n",
    "Making BERT embeddings takes long time for whole dataset, as the embeddings are context based and we can not use ready vectors for each word. So, model saves embeddings after the first time model finding embeddings. Each time for training new model it just uses the saved model.\n",
    "\n",
    "For comparision we compared our results with two baselines of the contest and top 5 competitors models. Considering F1 score, GRU using BERT pretrained embeddings could achieve second place in this contest!\n",
    "\n",
    "Two baselines of the contest are random binary model, which just randomly specifies the classes, and SVM model using tf-idf vectors. Top 5 competitors used some models and several techniques to improve their results. A short explanation of their works to improving the reults has came in \"SemEval-2018 Task 3: Irony Detection in English Tweets\" report.\n",
    "\n",
    "We used a GTX 1080 Ti for training our models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran our experiment 5 times for each method and reported the best one.\n",
    "Here our methods are TCN, GRU, GRU-bidirection, LSTM and LSTM-bidirection. Our methods are reported on both BERT and Glove language models. In addition, for the comparison, we have included the top 5 models and baseline methods from the competition as well.\n",
    "The methods were sorted based on F1 score in the competition. We also followed this convention for our methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border = 1> <tr><td>method</td><td>acc</td><td>precision</td><td>recall</td><td>F1</td></tr><tr><td>THU_NGN</td><td>0.735</td><td>0.630</td><td>0.801</td><td>0.705</td></tr><tr><td>NTUA-SLP</td><td>0.732</td><td>0.654</td><td>0.691</td><td>0.672</td></tr><tr><td>WLV</td><td>0.643</td><td>0.532</td><td>0.836</td><td>0.650</td></tr><tr><td>NLRPL-IITBHU</td><td>0.661</td><td>0.551</td><td>0.788</td><td>0.648</td></tr><tr><td>NIHRIO</td><td>0.702</td><td>0.609</td><td>0.691</td><td>0.648</td></tr><tr><td>GRU_BERT</td><td>0.737</td><td>0.653</td><td>0.702</td><td>0.685</td></tr><tr><td>Bi_GRU_BERT</td><td>0.731</td><td>0.663</td><td>0.563</td><td>0.658</td></tr><tr><td>LSTM_BERT</td><td>0.739</td><td>0.702</td><td>0.591</td><td>0.642</td></tr><tr><td>TCN_BERT</td><td>0.673</td><td>0.571</td><td>0.714</td><td>0.634</td></tr><tr><td>Bi_LSTM_BERT</td><td>0.726</td><td>0.692</td><td>0.556</td><td>0.617</td></tr><tr><td>GRU_Glove</td><td>0.666</td><td>0.562</td><td>0.711</td><td>0.628</td></tr><tr><td>LSTM_Glove</td><td>0.682</td><td>0.591</td><td>0.650</td><td>0.619</td></tr><tr><td>Bi_GRU_Glove</td><td>0.681</td><td>0.588</td><td>0.653</td><td>0.619</td></tr><tr><td>Bi_LSTM_Glove</td><td>0.684</td><td>0.603</td><td>0.592</td><td>0.597</td></tr><tr><td>TCN_Glove</td><td>0.634</td><td>0.531</td><td>0.662</td><td>0.589</td></tr><tr><td>Unigram SVM</td><td>0.635</td><td>0.532</td><td>0.659</td><td>0.589</td></tr><tr><td>Random</td><td>0.503</td><td>0.373</td><td>0.373</td><td>0.373</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [['method', 'acc', 'precision', 'recall', 'F1'],\n",
    "        ['THU_NGN', '0.735', '0.630', '0.801', '0.705'],\n",
    "        ['NTUA-SLP', '0.732' , '0.654', '0.691', '0.672'],\n",
    "        \n",
    "        ['WLV' ,'0.643', '0.532', '0.836', '0.650'],\n",
    "        \n",
    "        ['NLRPL-IITBHU', '0.661', '0.551', '0.788', '0.648'],\n",
    "        \n",
    "        ['NIHRIO', '0.702', '0.609', '0.691', '0.648'],\n",
    "        \n",
    "        ['GRU_BERT', '0.737', '0.653', '0.702' ,'0.685'],\n",
    "        ['Bi_GRU_BERT', '0.731' ,'0.663', '0.563', '0.658'],\n",
    "        ['LSTM_BERT' ,'0.739' , '0.702', '0.591' , '0.642'],\n",
    "        ['TCN_BERT' ,'0.673' , '0.571' , '0.714' , '0.634'],\n",
    "        ['Bi_LSTM_BERT', '0.726', '0.692', '0.556', '0.617'],\n",
    "        \n",
    "        ['GRU_Glove', '0.666', '0.562', '0.711', '0.628'],\n",
    "        ['LSTM_Glove', '0.682', '0.591', '0.650', '0.619'],\n",
    "        ['Bi_GRU_Glove', '0.681', '0.588' ,'0.653', '0.619'],\n",
    "        ['Bi_LSTM_Glove', '0.684', '0.603', '0.592' ,'0.597'],\n",
    "        ['TCN_Glove', '0.634', '0.531', '0.662', '0.589'],\n",
    "        ['Unigram SVM', '0.635', '0.532', '0.659', '0.589'],\n",
    "        ['Random', '0.503', '0.373', '0.373', '0.373'],\n",
    "       ]\n",
    "display(HTML(\n",
    "    #'<html><head><style>table, th, td {border: 1px solid black;}</style></head><body>'\n",
    "   \"<table border = 1> <tr>{}</tr></table>\".format(\n",
    "       '</tr><tr>'.join(\n",
    "           '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data)\n",
    "       )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is shown in the table above, our GRU and LSTM models based on BERT language model have achieved the best accuracy among the competition models. Moreover, we have the best precision with LSTM on BERT language model. For the recall and F1 score we didn't beat the best model, but we have achieved the F1 score to get the second place in the competition.\n",
    "The reason for the lower F1 score could be having low recall score. It seems that achieving better recall values would be feasible with some parameter tuning, which we did not have so much time to do it perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we used pre-trained embeddings and compared TCN and some variations of RNN models for detecting irony tweets. For the future we have some ideas to augment our data by adding or removing some unimportant words. We can also use attention to specify the importance of hidden units we use to generate the output. We could also concatenate embeddings from different langauge models to capture differnt embeddings in various contexts and types of sentences, but it needs a good tokenizer that can work for different language models, which seems hard and time consuming, and we didn't have the oppurtunity to check this direction.At last, as mentioned, parameter fine-tuning is also a good method to improve the performance, but again we did not have so much time to investigate it completely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
