{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran our experiment 5 times for each method and reported the best one.\n",
    "Here our methods are TCN, GRU, GRU-bidirection, LSTM and LSTM-bidirection. Our methods are reported on both BERT and Glove language models. In addition, for the comparison, we have included the top 5 models and baseline methods from the competition as well.\n",
    "The methods were sorted based on F1 score in the competition. We also followed this convention for our methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border = 1> <tr><td>method</td><td>acc</td><td>precision</td><td>recall</td><td>F1</td></tr><tr><td>THU_NGN</td><td>0.735</td><td>0.630</td><td>0.801</td><td>0.705</td></tr><tr><td>NTUA-SLP</td><td>0.732</td><td>0.654</td><td>0.691</td><td>0.672</td></tr><tr><td>WLV</td><td>0.643</td><td>0.532</td><td>0.836</td><td>0.650</td></tr><tr><td>NLRPL-IITBHU</td><td>0.661</td><td>0.551</td><td>0.788</td><td>0.648</td></tr><tr><td>NIHRIO</td><td>0.702</td><td>0.609</td><td>0.691</td><td>0.648</td></tr><tr><td>GRU_BERT</td><td>0.737</td><td>0.653</td><td>0.702</td><td>0.685</td></tr><tr><td>Bi_GRU_BERT</td><td>0.731</td><td>0.663</td><td>0.563</td><td>0.658</td></tr><tr><td>LSTM_BERT</td><td>0.739</td><td>0.702</td><td>0.591</td><td>0.642</td></tr><tr><td>TCN_BERT</td><td>0.673</td><td>0.571</td><td>0.714</td><td>0.634</td></tr><tr><td>Bi_LSTM_BERT</td><td>0.726</td><td>0.692</td><td>0.556</td><td>0.617</td></tr><tr><td>GRU_Glove</td><td>0.666</td><td>0.562</td><td>0.711</td><td>0.628</td></tr><tr><td>LSTM_Glove</td><td>0.682</td><td>0.591</td><td>0.650</td><td>0.619</td></tr><tr><td>Bi_GRU_Glove</td><td>0.681</td><td>0.588</td><td>0.653</td><td>0.619</td></tr><tr><td>Bi_LSTM_Glove</td><td>0.684</td><td>0.603</td><td>0.592</td><td>0.597</td></tr><tr><td>TCN_Glove</td><td>0.634</td><td>0.531</td><td>0.662</td><td>0.589</td></tr><tr><td>Unigram SVM</td><td>0.635</td><td>0.532</td><td>0.659</td><td>0.589</td></tr><tr><td>Random</td><td>0.503</td><td>0.373</td><td>0.373</td><td>0.373</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [['method', 'acc', 'precision', 'recall', 'F1'],\n",
    "        ['THU_NGN', '0.735', '0.630', '0.801', '0.705'],\n",
    "        ['NTUA-SLP', '0.732' , '0.654', '0.691', '0.672'],\n",
    "        \n",
    "        ['WLV' ,'0.643', '0.532', '0.836', '0.650'],\n",
    "        \n",
    "        ['NLRPL-IITBHU', '0.661', '0.551', '0.788', '0.648'],\n",
    "        \n",
    "        ['NIHRIO', '0.702', '0.609', '0.691', '0.648'],\n",
    "        \n",
    "        ['GRU_BERT', '0.737', '0.653', '0.702' ,'0.685'],\n",
    "        ['Bi_GRU_BERT', '0.731' ,'0.663', '0.563', '0.658'],\n",
    "        ['LSTM_BERT' ,'0.739' , '0.702', '0.591' , '0.642'],\n",
    "        ['TCN_BERT' ,'0.673' , '0.571' , '0.714' , '0.634'],\n",
    "        ['Bi_LSTM_BERT', '0.726', '0.692', '0.556', '0.617'],\n",
    "        \n",
    "        ['GRU_Glove', '0.666', '0.562', '0.711', '0.628'],\n",
    "        ['LSTM_Glove', '0.682', '0.591', '0.650', '0.619'],\n",
    "        ['Bi_GRU_Glove', '0.681', '0.588' ,'0.653', '0.619'],\n",
    "        ['Bi_LSTM_Glove', '0.684', '0.603', '0.592' ,'0.597'],\n",
    "        ['TCN_Glove', '0.634', '0.531', '0.662', '0.589'],\n",
    "        ['Unigram SVM', '0.635', '0.532', '0.659', '0.589'],\n",
    "        ['Random', '0.503', '0.373', '0.373', '0.373'],\n",
    "       ]\n",
    "display(HTML(\n",
    "    #'<html><head><style>table, th, td {border: 1px solid black;}</style></head><body>'\n",
    "   \"<table border = 1> <tr>{}</tr></table>\".format(\n",
    "       '</tr><tr>'.join(\n",
    "           '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data)\n",
    "       )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is shown in the table above, our GRU and LSTM models based on BERT language model have achieved the best accuracy among the competition models. Moreover, we have the best precision with LSTM on BERT language model. For the recall and F1 score we didn't beat the best model, but we have achieved the F1 score to get the second place in the competition.\n",
    "The reason for the lower F1 score could be having low recall score. It seems that achieving better recall values would be feasible with some parameter tuning, which we didn't have so much time to do it perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we used pre-trained embeddings and compared TCN and some variations of RNN models for detecting irony tweets. For the future we have some ideas to augment our data by adding or removing some unimportant words. We can also use attention to specify the importance of hidden units we use to generate the output. We could also concatenate embeddings from different langauge models to capture differnt embeddings in various contexts and types of sentences, but it needs a good tokenizer that can work for different language models, which seems hard and time consuming, and we didn't have the oppurtunity to check this direction.At last, as mentioned, parameter fine-tuning is also a good method to improve the performance, but again we didn't have so much time to investigate it completely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
